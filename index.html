<!DOCTYPE HTML>
<html lang="en">
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-163784922-1"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};
        //JS: "||" is OR function, push() is append()
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

gtag('config', 'UA-163784922-1');
    </script>
    <title>Yash Kant</title>
    <meta name="author" content="Yash Kant">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link rel="icon" type="image/png" href="data/yashcircle.jpg">
</head>

<body>
<table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
    <tr style="padding:0px">
        <td style="padding:0px">
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <tr style="padding:0px">
                    <td style="padding:2.5%;width:63%;vertical-align:middle">
                        <p style="text-align:center">
                            <name>Yash Kant</name>
                        </p>
                        <p>I am a Research Visitor at Georgia Tech advised by <a
                                href="https://www.cc.gatech.edu/~parikh/">Devi Parikh</a> and <a
                                href="https://www.cc.gatech.edu/~dbatra/">Dhruv Batra</a>.
                            I closely collaborate with <a href="https://dexter1691.github.io">Harsh Agrawal</a>.
                        </p>

                        <p>
                            I want to build machines that can see, interpret, and interact with the world reasonably.
                            Presently, I am building embodied agents that specialize in doing day-to-day tasks by
                            building a more human-like representation of their environments.

                        </p>

                        <p>I finished my undergraduate studies from <a href="http://iitr.ac.in">Indian Institute of
                            Technology Roorkee</a>. I have interned at <a
                                href="https://www.microsoft.com/en-in/msidc/bangalore-campus.aspx">Microsoft,
                            Bangalore</a> and visited <a href="http://www.nus.edu.sg/">National University of
                            Singapore</a> twice as a research assistant.
                        <p>
                        <p>
                            <strong>
                                I am actively looking for internship positions for Summer 2021.
                                I will be applying to PhD programs for Fall 2021.
                            </strong>
                        </p>

                        <p style="text-align:center">
                            <a href="mailto:ysh.kant@gmail.com">Email</a> &nbsp/&nbsp
                            <a href="data/cv-yashkant.pdf"
                               onclick="ga('send', 'event', 'Videos', 'play', 'Fall Campaign')">CV</a> &nbsp/&nbsp
                            <a href="https://github.com/yashkant"> Github </a> &nbsp/&nbsp
                            <a href="https://scholar.google.com/citations?user=rwNKTYIAAAAJ&hl=en"> Google Scholar </a> &nbsp/&nbsp
                            <a href="https://twitter.com/yash2kant">Twitter</a> &nbsp/&nbsp
                            <a href="https://in.linkedin.com/in/yash-kant/"> LinkedIn </a>
                        </p>
                    </td>
                    <td style="padding:2.5%;width:40%;max-width:40%">
                        <a href="data/yashkant.jpg"><img style="width:100%;max-width:100%" alt="profile photo"
                                                         src="data/yashkant.jpg" class="hoverZoomLink"></a>
                    </td>
                </tr>
                </tbody>
            </table>


            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <tr>
                    <td style="padding:20px;width:100%;vertical-align:middle">
                        <heading>News</heading>
                        <ul>

                            <li> [Oct 2020] ConCAT accepted to <a href="https://sslneuips20.github.io/">NeurIPS 2020 SSL
                                workshop!</a></li>
                            <li> [Oct 2020] ConCAT on arxiv: <a href="https://arxiv.org/abs/2010.06087">Contrast and
                                Classify: Alternate Training for Robust VQA</a></li>
                            <li> [Sep 2020] Lead organizer of <a href="https://textvqa.org/challenge">TextVQA</a>
                                and co-organizer of <a href="https://textvqa.org/textcaps/challenge">TextCaps</a> and <a
                                        href="https://visualqa.org/workshop.html">VQA</a> 2021 challenges.
                            <li> [Jul 2020] Runners-up of the <a href="https://textvqa.org/challenge">TextVQA</a>
                                challenge organized at <a href="https://visualqa.org/workshop.html">CVPR 2020!</a>
                            </li>
                            <li> [Jul 2020] SAM accepted to ECCV, 2020 and CVPR 2020 VQA workshop!</a></li>
                            <li> [Jul 2020] SAM on arxiv: <a href="https://arxiv.org/abs/2007.12146">Spatially Aware
                                Multimodal Transformers for TextVQA</a></li>
                            <li> [Jul 2019] Revamped the <a href="http://demo.visualdialog.org/">Visual Chatbot</a>
                                with a complete rewrite of the old Lua-Torch codebase.
                            </li>
                            <li> [May 2019] I will be visiting Georgia Tech working with Devi Parikh and Dhruv Batra.
                            </li>
                        </ul>
                    </td>
                </tr>
                </tbody>
            </table>

            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <tr>
                    <td style="padding-left:20px;padding-right:20px;padding-top:20px;width:25%;vertical-align:middle">
                        <heading>Research</heading>
                    </td>
                </tr>


                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src="data/concat-vqa/teaser-new.png">
                        </div>
                    </td>

                    <td style="padding:20px;width:100%;vertical-align:middle">
                        <a href="https://yashkant.github.io/projects/concat-vqa.html">
                            <papertitle>Contrast and Classify: Alternate Training for Robust VQA</papertitle>
                        </a>
                        <br>
                        <strong>Yash Kant</strong>,
                        <a href="https://amoudgl.github.io/">Abhinav Moudgil</a>,
                        <a href="https://www.cc.gatech.edu/~dbatra/">Dhruv Batra</a>,
                        <a href="https://www.cc.gatech.edu/~parikh/">Devi Parikh</a>,
                        <a href="https://dexter1691.github.io/">Harsh Agrawal</a>
                        <br>
                        <em>Under Review / Self-Supervised Learning Workshop at NeurIPS, 2020</em>
                        <br>
                        <a href="https://arxiv.org/abs/2010.06087">arXiv</a> /
                        <a href="https://yashkant.github.io/projects/concat-vqa.html">project page</a> /
                        <a href="https://github.com/yashkant/concat-vqa">code</a> /
                        <a href="https://yashkant.github.io/data/concat-vqa/slides.pdf">slides</a>
                        <br>
                        <p></p>
                        <p>We propose a novel training paradigm (ConCAT) that alternately optimizes cross-entropy and
                            contrastive losses.
                            The contrastive loss encourages representations to be robust to linguistic variations in
                            questions while
                            the cross-entropy loss preserves the discriminative power of the representations for answer
                            classification.</p>
                    </td>
                </tr>


                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <div class="two" id="lh_image" style="opacity: 0;">
                                <video muted="" autoplay="" loop="" width="100%" height="100%">
                                    <source src="data/sam-textvqa/textvqa-workshop.gif" type="video/mp4">
                                    Your browser does not support the video tag.
                                </video>
                            </div>
                            <img src="data/sam-textvqa/textvqa-workshop.gif">
                        </div>
                    </td>

                    <td style="padding:20px;width:100%;vertical-align:middle">
                        <a href="https://yashkant.github.io/projects/sam-textvqa.html">
                            <papertitle>Spatially Aware Multimodal Transformers for TextVQA</papertitle>
                        </a>
                        <br>
                        <strong>Yash Kant</strong>,
                        <a href="https://www.cc.gatech.edu/~dbatra/">Dhruv Batra</a>,
                        <a href="https://panderson.me/">Peter Anderson</a>,
                        <a href="https://www.cc.gatech.edu/~jlu347/">Jiasen Lu</a>,
                        <a href="https://www.alexander-schwing.de/">Alexander Schwing</a>,
                        <a href="https://www.cc.gatech.edu/~parikh/">Devi Parikh</a>,
                        <a href="https://dexter1691.github.io/">Harsh Agrawal</a>
                        <br>
                        <em>ECCV, 2020 / VQA Workshop at CVPR, 2020</em>
                        <br>
                        <a href="https://arxiv.org/abs/2007.12146">arXiv</a> /
                        <a href="https://yashkant.github.io/projects/sam-textvqa.html">project page</a> /
                        <a href="https://github.com/yashkant/sam-textvqa">code</a> /
                        <a href="https://www.youtube.com/watch?v=uPZra6HfLd0">short talk</a> /
                        <a href="https://www.youtube.com/watch?v=rO89lcTvz2U">long talk</a> /
                        <a href="https://yashkant.github.io/data/sam-textvqa/slides.pdf">slides</a>
                        <br>
                        <p></p>
                        <p>We propose a novel spatially aware self-attention layer such that each visual entity only
                            looks
                            at neighboring entities defined by a spatial graph and use it to solve TextVQA.</p>
                    </td>
                </tr>
                </tbody>
            </table>


            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <tr>
                    <td style="padding:20px;width:100%;vertical-align:middle">
                        <heading>Projects</heading>
                        <!--              <p>My recent projects were</p>-->
                        <p class="content">
                        <ul>
                            <li><b><i>Adding Complement Objective Training to Pythia:</i></b> I experimented with adding
                                Complement Objective Training in FAIR's vision and language framework <a
                                        href="https://github.com/facebookresearch/pythia/">Pythia</a> and also wrote a
                                report on my findings <a
                                        href="https://drive.google.com/file/d/16NtLvZvBPq1cRVeCSq7sXXg0C8NkSi4l/view">here</a>,
                                the code is <a href="https://github.com/facebookresearch/pythia/pull/32">here</a>.
                            </li>
                            <li><b><i>ICLR Reproducibility Challenge:</i></b> We reproduced <a
                                    href="https://arxiv.org/abs/1806.06763">Closing the Generalization Gap of Adaptive
                                Gradient Methods in Training Deep Neural Networks </a> and here's the <a
                                    href="https://github.com/yashkant/Padam-Tensorflow">code.</a></li>
                            <li><b><i>Visual Chatbot Version 2.0 (<a
                                    href="https://github.com/Cloud-CV/visual-chatbot/pull/18">code here</a>):</i></b> I
                                shifted the old Lua-Torch codebase to PyTorch, added better captioning and trained the
                                VisDial model on BUTD features.
                            </li>
                            <li><b><i>Quantized Neural Architecture Search:</i></b> I quantized the search
                                space of Neural Architecture Search algorithms (<a
                                        href="https://arxiv.org/abs/1802.03268">ENAS</a>, 
                                    <a href="https://arxiv.org/abs/1712.00559">PNAS</a>) to search for
                                resource-efficient models. Code is <a href="https://github.com/yashkant/ENAS-Quantized-Neural-Networks">here (ENAS)</a>, 
                                and <a href="https://github.com/yashkant/PNAS-Binarized-Neural-Networks">here (PNAS).</a>
                            </li>
                        </ul>
                        </p>
                    </td>
                </tr>
                </tbody>
            </table>

            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <tr>
                    <td style="padding:0px">
                        <br>
                        <p style="text-align:right;font-size:small;">
                            I borrowed this template from Jon Barron's<a
                                href="https://github.com/jonbarron/jonbarron_website"> website</a>.
                            <br>
                        </p>
                    </td>
                </tr>
                </tbody>
            </table>
        </td>
    </tr>
</table>
</body>

</html>
